{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File location and type\n",
    "file_location = \"/FileStore/tables/clean_tripdata.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing the top 10 bikes with most distance traveled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, asc, desc\n",
    "\n",
    "# Calculate the total distance for each bike ID and sort in descending order\n",
    "df1 = df.groupBy(\"bike_id\").sum(\"distance\").withColumnRenamed(\"sum(distance)\", \"total_distance\").sort(desc(\"total_distance\"))\n",
    "\n",
    "# Get the top 10 sum of distances\n",
    "top_10_distances = df1.limit(10).select(\"total_distance\").collect()\n",
    "\n",
    "# Add a boolean column indicating if the bike ID has one of the top 10 sums of distance\n",
    "df1 = df1.withColumn(\"is_top_10_distance\", col(\"total_distance\").isin([row.total_distance for row in top_10_distances]))\n",
    "\n",
    "display(df1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longest trip calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "top_10_distances = df.orderBy(desc(\"distance\")).limit(10)\n",
    "display(top_10_distances.select(\"bike_id\", \"distance\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trip duration vs user type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a new column containing the duration in minutes to make the results clearer\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import round\n",
    "df_duration_minute = df.withColumn('duration_minutes', round(col('duration_seconds')/60).cast(IntegerType()))\n",
    "display(df_duration_minute)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Usage Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Remove outliers\n",
    "outliers = df.orderBy(desc(\"duration_seconds\")).limit(4)\n",
    "df_cluster = df.subtract(outliers)\n",
    "\n",
    "# Preprocess for K-means\n",
    "vec_assembler = VectorAssembler(inputCols=[\"duration_seconds\"], outputCol=\"features\")\n",
    "df_cluster = vec_assembler.transform(df_cluster)\n",
    "display(df_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(featuresCol=\"features\", k=3)  # initialize the algorithm\n",
    "model = kmeans.fit(df_cluster)  # fit the algorithm to our data\n",
    "\n",
    "# Assign cluster labels to the data\n",
    "predictions = model.transform(df_cluster)\n",
    "\n",
    "# Transform the outliers\n",
    "outliers = vec_assembler.transform(outliers)\n",
    "outlier_predictions = model.transform(outliers)\n",
    "\n",
    "#Append outliers to predictions\n",
    "predictions = predictions.union(outlier_predictions)\n",
    "\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Different Pricing systems\n",
    "\n",
    "First trying Linear Programming using PuLP then using a grid search-like method using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pulp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_counts = predictions.groupBy('prediction').count()\n",
    "num_trips = [row['count'] for row in cluster_counts.collect()]\n",
    "num_trips\n",
    "\n",
    "from pulp import LpMaximize, LpProblem, LpStatus, lpSum, LpVariable\n",
    "\n",
    "epsilon = 0.05\n",
    "\n",
    "# Create the model\n",
    "model = LpProblem(name = \"revenue-maximization\", sense = LpMaximize)\n",
    "\n",
    "# Initialize the decision variables: the prices for the tree clusters\n",
    "\n",
    "prices = [LpVariable(name = f\"price_{i}\", lowBound= 0.2, upBound=0.4) for i in range(3)]\n",
    "\n",
    "# Add the constraints to maintain the price order\n",
    "model += (prices[0] >= prices[1] + epsilon)\n",
    "model += (prices[1] >= prices[2] + epsilon)\n",
    "\n",
    "average_durations = [794.21, 60252.48, 15831.98]   #using cluster centers for average durations\n",
    "\n",
    "# Add the objective function: total revenue = sum(price * average_durations * num_trips for each cluster)\n",
    "model += lpSum([prices[i] * average_durations[i] * num_trips[i] for i in range(3)])\n",
    "\n",
    "# Solve the problem\n",
    "status = model.solve()\n",
    "\n",
    "# Get the results\n",
    "print(f\"status: {model.status}, {LpStatus[model.status]}\")\n",
    "print(f\"objective: {model.objective.value()}\")\n",
    "\n",
    "for var in prices:\n",
    "    print(f\"{var.name}: {var.value()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assume these are your initial prices\n",
    "prices = np.array([0.35, 0.30, 0.25])\n",
    "\n",
    "# Assume these are the average durations (in minutes) for each cluster\n",
    "average_durations = np.array([794.21, 60252.48, 15831.98])  # You should replace these with your actual data\n",
    "\n",
    "# Assume these are the number of trips in each cluster\n",
    "num_trips = np.array([1128, 6082, 512490])  # You should replace these with your actual data\n",
    "\n",
    "# Compute the initial revenue\n",
    "revenue = np.sum(prices * average_durations * num_trips)\n",
    "\n",
    "# Define a range of price adjustments\n",
    "adjustments = np.array([-0.05, 0, 0.05])\n",
    "\n",
    "# Initialize the best revenue to the initial revenue\n",
    "best_revenue = revenue\n",
    "best_prices = prices\n",
    "\n",
    "# Iterate over all combinations of price adjustments for the three clusters\n",
    "for adj1 in adjustments:\n",
    "    for adj2 in adjustments:\n",
    "        for adj3 in adjustments:\n",
    "            # Compute the adjusted prices\n",
    "            adjusted_prices = prices + np.array([adj1, adj2, adj3])\n",
    "            \n",
    "            # Ensure the price order\n",
    "            if adjusted_prices[0] >= adjusted_prices[1] >= adjusted_prices[2]:\n",
    "                # Compute the revenue with the adjusted prices\n",
    "                adjusted_revenue = np.sum(adjusted_prices * average_durations * num_trips)\n",
    "                \n",
    "                # If the adjusted revenue is better, update the best revenue and the best prices\n",
    "                if adjusted_revenue > best_revenue:\n",
    "                    best_revenue = adjusted_revenue\n",
    "                    best_prices = adjusted_prices\n",
    "\n",
    "print('Optimal prices:', best_prices)\n",
    "print('Optimal revenue:', best_revenue)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying the most popular routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "df_routes = df.select(\"*\")\n",
    "df_routes = df_routes.withColumn(\"route_id\", concat_ws(' --> ',df.start_station_id,df.end_station_id)) # this column contains the ids for the start and en stations combined\n",
    "df_routes = df_routes.withColumn(\"route\", concat_ws(' --> ',df.start_station_name,df.end_station_name)) # this column contains the namess for the start and en stations combined\n",
    "display(df_routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the top 10 routes\n",
    "df_top_routes = df_routes.groupBy(\"route\").count().orderBy(col(\"count\").desc()).limit(10)\n",
    "display(df_top_routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_worst_routes = df_routes.groupBy(\"route\").count().orderBy(col(\"count\").asc()).limit(10)\n",
    "display(df_worst_routes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonal usage patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a new column that contains the season\n",
    "from pyspark.sql.functions import when, lit \n",
    "df_seasons = df.withColumn(\"season\", \\\n",
    "      when((df.start_date >= \"2017-03-01\") & (df.start_date <= \"2017-05-31\"), lit(\"Spring\")) \\\n",
    "     .when((df.start_date >= \"2017-06-01\") & (df.start_date <= \"2017-08-31\"), lit(\"Summer\")) \\\n",
    "     .when((df.start_date >= \"2017-09-01\") & (df.start_date <= \"2017-11-30\"), lit(\"Fall (Autumn)\")) \\\n",
    "     .otherwise(lit(\"Winter\")) \\\n",
    "  )\n",
    "display(df_seasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max, min\n",
    "df.select(min('start_date'), max('start_date')).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter((df.start_date >= \"2017-03-01\") & (df.start_date <= \"2017-05-31\")).display() #for some reason there is no result for spring season\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaining the number of trips per season\n",
    "seasons_pd = df_seasons.groupBy(\"season\").count()\n",
    "display(seasons_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
